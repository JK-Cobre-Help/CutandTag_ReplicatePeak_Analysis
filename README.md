# CutandTag_ReplicatePeak_Analysis

![ReplicatePeaks](/images/replicatePeaks.png)
+ OpenAI. (2024). Scientific data visualization: Replicate peak analysis in bioinformatics [AI-generated image]. DALL-E. Retrieved from ChatGPT interface.

# 1) Project Description
CutAndTag_ReplicatePeak_Analysis is a Snakemake pipeline designed to perform downstream peak analysis on processed Cut-and-Tag sequencing data. Rather than starting from raw FASTQ reads, this pipeline starts with already aligned and filtered BAM files, focusing on the identification of reproducible peaks, the generation of consensus peak sets, and the visualization of overlaps and signal distributions across multiple samples or experimental conditions.

+ Note: If you are starting from raw FASTQ files, consider using the [CutandTag_Analysis_Snakemake](https://github.com/JK-Cobre-Help/CutandTag_Analysis_Snakemake) pipeline first. That pipeline handles the initial data processing steps—such as quality control, alignment, and basic filtering—providing you with the cleaned and aligned data that serve as the input for CutAndTag_ReplicatePeak_Analysis.

## Key Features
+ Peak Calling with MACS2:
    + The pipeline calls peaks for each sample using MACS2, a widely-used tool for identifying enriched regions in ChIP- or Cut-and-Tag sequencing data.

+ Merged and Consensus Peak Sets:
    + Using sample groupings defined in samples.csv (particularly the "Set" column), peaks are merged to produce a comprehensive candidate region set. A consensus peak set is then generated by applying a reproducibility threshold, ensuring that only peaks observed in a configurable minimum number of samples are retained.

+ Consensus Peak Conversion:
    + Consensus peak sets are converted into BAM and BigWig formats, enabling efficient genome browser visualization and facilitating downstream analyses.

+ Euler Plots of Overlaps:
    + The pipeline creates Euler diagrams to represent the overlap of peaks among individual samples within a set. This visual approach reveals how consensus peaks emerge from the intersection of multiple replicates.

+ Midpoint and Overlap Analysis:
    + The pipeline identifies peak midpoints and quantifies overlaps, enabling a more granular exploration of peak distribution and subtle differences or similarities across samples.

+ Heatmaps for Signal Distribution:
    + By using consensus peak midpoints, the pipeline generates heatmaps that visualize aggregated signal (coverage) patterns. These heatmaps provide insights into the intensity and distribution of signal across multiple conditions or sample sets.

# 2) Intended Use Case
This pipeline is ideal for researchers who have already processed their Cut-and-Tag data through preliminary steps such as quality control, alignment, and filtering (e.g., by using [CutandTag_Analysis_Snakemake](https://github.com/JK-Cobre-Help/CutandTag_Analysis_Snakemake)). After obtaining high-quality aligned BAM files, you can use CutAndTag_ReplicatePeak_Analysis to:

+ Identify reproducible peaks across replicates or experimental conditions.
+ Generate integrative visual summaries of peak overlaps.
+ Compare signal intensity profiles around consensus peak midpoints.

By integrating this two-step approach, you ensure a robust, end-to-end workflow for your Cut-and-Tag sequencing experiments.

# 3) Dependencies and Configuration
Configuration:
All parameters (e.g., genome size, MACS2 q-values, minimum number of overlapping samples for consensus peaks, paths to executables) are controlled via the config/config.yml file.
## Explanation of config.yml
+ Note. Make sure to check config.yml for the appropriate genome alignment

The config.yml file in this repository controls most of the parameters and references used by the pipeline, including genome settings, tool versions, and other workflow parameters.

Genome and Effective Genome Size
The pipeline uses the genome and effective_genome_size parameters to configure tools like macs2 and deeptools for your organism of interest. For instance:

For human (hg38), set genome: "hs" and effective_genome_size: 2913022398.
For mouse (mm10), set genome: "mm" and effective_genome_size: 2730871774.
By default, the config.yml is set up for hs (human hg38). Running mouse (mm10) samples requires changing these values to match the mm10 parameters, which are already provided in config.yml as comments.

## Changing Genomes
To switch from mm10 to hg38 (or vice versa), you’ll need to:

+ Update the effective_genome_size:
    + Change this parameter to the appropriate value for your chosen genome. For hg38, use 2913022398. For mm10, use 2730871774.

+ Update the chrom_sizes File:
    + Point chrom_sizes to the correct chromosome sizes file, such as resources/hg38.chrom.sizes for hg38 or resources/mm10.chrom.sizes for mm10.

+ Update the genome Parameter for MACS2:
    + Set the genome parameter to "hs" for human or "mm" for mouse, to ensure that MACS2 uses the correct genome size estimates.

All information required for switching between hg38 and mm10 is included in config.yml, commented out next to the default settings. Simply uncomment and modify these values as needed when changing the genome from mm10 to hg38.

Tool Versions and Modules
The config.yml file also specifies versions of tools and modules (e.g., deeptools, macs2, samtools, bedtools, R) used by the pipeline. These versions help maintain reproducibility and ensure that the pipeline runs consistently across different computing environments.

# 4) Tools & Modules:
The pipeline relies on a series of bioinformatics tools, including:

+ MACS2 for peak calling
+ bedtools and samtools for peak and alignment format conversions
+ deeptools for coverage and matrix computation, as well as for generating heatmaps
+ R with Bioconductor packages for merging peaks, generating consensus sets, and creating Euler diagrams

# 5) Example Data
A compact, pre-processed dataset is included in this repository to quickly test the pipeline and validate that your environment is set up correctly. This small example replicates the pipeline’s key steps from peak calling through to final visualization.

## Relationship to Previous Protocols
This pipeline builds on the framework originally described in CutandTag_Analysis_Snakemake, which was designed to process raw FASTQ files. In contrast, CutAndTag_ReplicatePeak_Analysis focuses on downstream analysis steps, taking already processed and aligned data to produce detailed consensus peak sets and visual summaries, thus facilitating more advanced and reproducible analyses.


# 6) Explanation of samples.csv
Note. Make sure to check sample.csv before each run

The samples.csv file in the config folder has paths to the test fastq files. You must replace those paths with those for your own fastq files. The first column of each row is the sample name. This name will be used for all output files. Columns 2 and 3 are the paths to the paired fastq files. Column 4 is the sample type (either "treatment" or "control"). Column 5 is the name of the corresponding Control sample for each treated sample (use "NA" if the sample is a control).

| sample             | fastq1                        | fastq2                        | sampleType | Control   |
|--------------------|-------------------------------|-------------------------------|------------|-----------|
| K27ac_50_trimmed   | K27ac_50_trimmed_R1.fastq.gz  | K27ac_50_trimmed_R2.fastq.gz  | control    | NA        |
| K27me3_50_trimmed  | K27me3_50_trimmed_R1.fastq.gz | K27me3_50_trimmed_R1.fastq.gz | control    | NA        |


Sample naming recommendation for correct plot output
- "Histone" + "_" + "Replicate" + "Any other identifier"
- Examples:
    + K27ac_50
    + K27me3_5
    + K27ac_50_trimmed
    + H3K27me3_rep1
    + H3K4me3_rep2_set1
    + H3K27ac_rep3_control
    + H3K27ac_rep3_treatment

# 7) Instructions to run on Slurm managed HPC
2A. Clone repository
```
git clone https://github.com/JK-Cobre-Help/CutandTag_ReplicatePeak_Analysis.git
```
2B. Load modules
```
module purge
module load slurm python/3.10 pandas/2.2.3 numpy/1.22.3 matplotlib/3.7.1
```
2C. Modify samples and config file
```
vim samples.csv
vim config.yml
```
2D. Dry Run
```
snakemake -npr
```
2E. Run on HPC with config.yml options
```
sbatch --wrap="snakemake -j 999 --use-envmodules --latency-wait 60 --cluster-config config/cluster_config.yml --cluster 'sbatch -A {cluster.account} -p {cluster.partition} --cpus-per-task {cluster.cpus-per-task}  -t {cluster.time} --mem {cluster.mem} --output {cluster.output}'"
```

